{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0d0c7b3-16ca-4576-ad82-85ace02c526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5fa759ca-7cfc-41bd-b0c1-d60ad8673622",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = {\n",
    "    'gpt-4o-2024-05-13':{\n",
    "        'input_price': 5/1000000, # 1M tokens\n",
    "        'output_price': 15/1000000 # 1M tokens\n",
    "    },\n",
    "    'gpt-4-turbo-2024-04-09':{\n",
    "        'input_price': 10/1000000, # 1M tokens\n",
    "        'output_price': 30/1000000 # 1M tokens\n",
    "    },\n",
    "    'gpt-4-0125-preview':{ #'gpt-4-1106-preview'\n",
    "        'input_price': 10/1000000, # 1M tokens\n",
    "        'output_price': 30/1000000 # 1M tokens\n",
    "    },\n",
    "    'gpt-4-0613':{\n",
    "        'input_price': 30/1000000, # 1M tokens\n",
    "        'output_price': 60/1000000 # 1M tokens\n",
    "    },\n",
    "    'gpt-3.5-turbo-0125':{\n",
    "        'input_price': 0.5/1000000, # 1M tokens\n",
    "        'output_price': 1.5/1000000 # 1M tokens\n",
    "    },\n",
    "    'gpt-3.5-turbo-1106':{\n",
    "        'input_price': 0.001/1000, # 1K tokens\n",
    "        'output_price': 0.002/1000 # 1K tokens\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede36339-a92a-4ee7-a8e6-7559111f74ac",
   "metadata": {},
   "source": [
    "# Prepare Reasoning Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03e300c0-6e5b-4a9d-865c-06e1095ae055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path: str) -> List[str]:\n",
    "    file_suffix = file_path.split('.')[-1]\n",
    "    file_data = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        if file_suffix == 'jsonl':\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                choices_text = ', '.join([f\"{choice['label']}. {choice['text']}\" for choice in data['question']['choices']]) #+ '.'\n",
    "                item = {\n",
    "                    'Q': data['question']['stem'],\n",
    "                    'Options': choices_text.strip(),\n",
    "                    'GT': data['answerKey'],\n",
    "                    'Other': [choice['label'] for choice in data['question']['choices'] if choice['label'] != data['answerKey']]\n",
    "                }\n",
    "                file_data.append(item)\n",
    "        elif file_suffix == 'json':\n",
    "            json_data = json.load(file)\n",
    "            for data in json_data:\n",
    "                item = {\n",
    "                    'Q': data['question'],\n",
    "                    'Options': 'A. yes, B. no',\n",
    "                    'GT': 'A' if data['answer'] else 'B',\n",
    "                    'Other': ['B' if data['answer'] else 'A']\n",
    "                }\n",
    "                file_data.append(item)\n",
    "                \n",
    "    return file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e567cf8-02f3-49f1-8cab-c0ab18a631f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning_prompt_system(examples: List[Dict[str, any]], request_prompt: Dict[str, any], num_shot: int = 4) -> (List[Dict[str, str]], List[str]):\n",
    "    examples = random.sample(examples, k=num_shot)\n",
    "    examples_message = []\n",
    "    reasoning_message = []\n",
    "    \n",
    "    for example in examples:\n",
    "        reasoning_content = '\\n'.join(example['Explanations'])\n",
    "        examples_message.append(\n",
    "            f\"\\n###\\n\"\n",
    "            f\"{example['Q']}\\n\"\n",
    "            f\"Options: {example['Options']}\\n\"\n",
    "            f\"Key Information: {example['Key Information']}\\n\" \n",
    "            f\"Explanations: {reasoning_content}\\n\"\n",
    "            f\"###\\n\"\n",
    "        )\n",
    "        reasoning_content = f\"Key Information: {example['Key Information']}\\nExplanations: \" + reasoning_content\n",
    "        reasoning_message.append(reasoning_content)\n",
    "        \n",
    "    examples_string = \"\".join(examples_message)\n",
    "    system_message = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"{request_prompt['Key']}{examples_string}\".strip()\n",
    "        },\n",
    "    ]\n",
    "    return system_message, reasoning_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf3651b9-f2ca-40d7-b94f-f1d915b02f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning_prompt_user(data: List[Dict[str, any]]) -> List[Dict[str, str]]:\n",
    "    user_message = []\n",
    "    for item in data:\n",
    "        option_content = \"\".join([f\"{option} is incorrect. Because\\n\" for option in item['Other']])\n",
    "        user_content = \"\".join([\n",
    "            f\"{item['Q']}\\n\",\n",
    "            f\"Options: {item['Options']}\\n\",\n",
    "            f\"Key Information:\\n\",\n",
    "            f\"Explanations: {item['GT']} is correct. Because\\n{option_content}\"\n",
    "        ]).strip()\n",
    "        user_query = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_content\n",
    "            },\n",
    "        ]\n",
    "        user_message.append(user_query)\n",
    "    return user_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a947ff7e-d54c-4f2b-af9e-343746f49a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages: List[Dict[str, str]], model: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-3.5-turbo-1106\",\n",
    "        'gpt-3.5-turbo-0125',\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-0125-preview\",\n",
    "        \"gpt-4-1106-preview\",\n",
    "        \"gpt-4-turbo-2024-04-09\",\n",
    "        \"gpt-4o-2024-05-13\"\n",
    "        }:\n",
    "        tokens_per_message = 3 # every message follows <|im_start|><im_sep>{content}<|im_end|>\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "        \n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        if isinstance(message, dict):\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":\n",
    "                    num_tokens += tokens_per_name\n",
    "        elif isinstance(message, str):\n",
    "            num_tokens += len(encoding.encode(message))\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant<im_sep>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c281d04-674e-4ed7-be9d-6ee9ada0ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_reasoning_prompt(\n",
    "    model_name, \n",
    "    system_message, \n",
    "    user_message,\n",
    "    few_shot_message,\n",
    "    num_prompts = None,\n",
    "    max_tokens = None,\n",
    "    num_sample = None,\n",
    "    temperature = 0.8,\n",
    "    save_path = None\n",
    "):\n",
    "    if num_prompts is None:\n",
    "        num_prompts = len(user_message)\n",
    "    user_message_subset = user_message[:num_prompts]\n",
    "\n",
    "    prompts = [{\n",
    "        'model': model_name,\n",
    "        'messages': system_message + message if system_message is not None else message,\n",
    "        'max_tokens': max_tokens,\n",
    "        'n': num_sample,\n",
    "        'temperature': temperature,\n",
    "    } for message in user_message_subset]\n",
    "  \n",
    "    estimate_price(system_message, user_message_subset, few_shot_message, num_sample, max_tokens)\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as file:\n",
    "            for prompt in prompts:\n",
    "                file.write(json.dumps(prompt) + '\\n')\n",
    "                \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e6a3236-4d95-4353-a92c-9ba2e6e473e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_price(\n",
    "    system_message, \n",
    "    user_message,\n",
    "    few_shot_message,\n",
    "    num_sample=1, \n",
    "    max_tokens=None\n",
    "):\n",
    "    for model_name in MODEL.keys():\n",
    "        print(f'{model_name}/{dataset_name}')\n",
    "        \n",
    "        input_price = MODEL[model_name]['input_price']\n",
    "        output_price = MODEL[model_name]['output_price']\n",
    "\n",
    "        # calculate input token\n",
    "        num_tokens_system_message = num_tokens_from_messages(system_message, model_name) if system_message else 0\n",
    "        len_input_token = [num_tokens_system_message + num_tokens_from_messages(item, model_name) for item in user_message]\n",
    "\n",
    "        input_mean_price = np.mean(len_input_token) * input_price\n",
    "        input_estimated_price = input_mean_price * len(user_message)\n",
    "        print(f'The mean length of GPT input is {np.mean(len_input_token)}')\n",
    "        \n",
    "        \n",
    "        #calculate output token\n",
    "        if max_tokens is None:\n",
    "            len_output_token = [num_tokens_from_messages([message], model_name) for message in few_shot_message]\n",
    "            output_mean_price = np.mean(len_output_token) * output_price\n",
    "            print(f'The mean length of output(*num_sample) is {np.mean(len_output_token) * num_sample}')\n",
    "            print(f'estimated tokens per request is {np.mean(len_input_token) + np.mean(len_output_token)}')\n",
    "        else:\n",
    "            output_mean_price = max_tokens * output_price\n",
    "            print(f'The max length of output is {max_tokens}')\n",
    "            print(f'estimated tokens per request is {np.mean(len_input_token) + max_tokens}')\n",
    "        \n",
    "        output_estimated_price = output_mean_price * num_sample * len(user_message)\n",
    "        \n",
    "        print(f'estimated input price is ${input_estimated_price}/${input_mean_price}')\n",
    "        print(f'estimated output price is ${output_estimated_price}/${output_mean_price}')\n",
    "        print(f'estimated final price is ${input_estimated_price+output_estimated_price}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b313c4-bb13-4469-9a45-8fd013c82c39",
   "metadata": {},
   "source": [
    "## CSQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec70352b-3c60-4e32-891e-0d6753572683",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"csqa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44c32b70-c416-4a5f-b39c-66e2ca39d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_data(f'{dataset_name}/original/train_rand_split.jsonl')\n",
    "validation_data = read_data(f'{dataset_name}/original/dev_rand_split.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4329d82a-4543-48b1-89f5-567826088574",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_validation_data = random.sample(training_data, k=len(validation_data))\n",
    "for item in new_validation_data:\n",
    "    training_data.remove(item)\n",
    "new_test_data = validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97af768d-3f14-4239-8d6d-2b453bb1d094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(f'{dataset_name}/prompt/{dataset_name}_cot.json', 'r') as file:\n",
    "    csqa_examples = json.load(file)\n",
    "\n",
    "with open('request.json', 'r') as file:\n",
    "    request_prompt = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "966088c4-6dea-49f0-854b-779c22a722fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "csqa_system_message, reasoning_message = reasoning_prompt_system(csqa_examples, request_prompt, num_shot=8)\n",
    "csqa_user_message = reasoning_prompt_user(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a1773d0-3592-449d-8e4a-2edeacf8354a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-2024-05-13/csqa\n",
      "The mean length of GPT input is 1777.4390845070423\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 2033.4390845070423\n",
      "estimated input price is $75.718905/$0.008887195422535212\n",
      "estimated output price is $261.7344/$0.00384\n",
      "estimated final price is $337.453305\n",
      "\n",
      "gpt-4-turbo-2024-04-09/csqa\n",
      "The mean length of GPT input is 1790.743896713615\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 2046.743896713615\n",
      "estimated input price is $152.57138/$0.01790743896713615\n",
      "estimated output price is $523.4688/$0.00768\n",
      "estimated final price is $676.04018\n",
      "\n",
      "gpt-4-0125-preview/csqa\n",
      "The mean length of GPT input is 1790.743896713615\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 2046.743896713615\n",
      "estimated input price is $152.57138/$0.01790743896713615\n",
      "estimated output price is $523.4688/$0.00768\n",
      "estimated final price is $676.04018\n",
      "\n",
      "gpt-4-0613/csqa\n",
      "The mean length of GPT input is 1790.743896713615\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 2046.743896713615\n",
      "estimated input price is $457.71414/$0.05372231690140845\n",
      "estimated output price is $1046.9376/$0.01536\n",
      "estimated final price is $1504.65174\n",
      "\n",
      "gpt-3.5-turbo-0125/csqa\n",
      "The mean length of GPT input is 1790.743896713615\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 2046.743896713615\n",
      "estimated input price is $7.628568999999999/$0.0008953719483568074\n",
      "estimated output price is $26.17344/$0.000384\n",
      "estimated final price is $33.802009\n",
      "\n",
      "gpt-3.5-turbo-1106/csqa\n",
      "The mean length of GPT input is 1790.743896713615\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 2046.743896713615\n",
      "estimated input price is $15.257137999999998/$0.0017907438967136148\n",
      "estimated output price is $34.89792/$0.000512\n",
      "estimated final price is $50.155058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csqa_example = wrap_reasoning_prompt(\n",
    "    model_name='gpt-4o-2024-05-13', \n",
    "    system_message=csqa_system_message, \n",
    "    user_message=csqa_user_message,\n",
    "    few_shot_message=reasoning_message,\n",
    "    num_prompts=None,\n",
    "    max_tokens=256,\n",
    "    num_sample=8,\n",
    "    temperature=0.8,\n",
    "    save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34d8ff80-c7c8-45b1-8ca6-1b89e6bad8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dataset_name}/prompt/valid_raw.json', 'w') as f:\n",
    "    json.dump(new_validation_data, f)\n",
    "\n",
    "with open(f'{dataset_name}/prompt/test_raw.json', 'w') as f:\n",
    "    json.dump(new_test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a6930-d6c3-44a8-af8f-02254c37c505",
   "metadata": {},
   "source": [
    "## StrategyQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8916454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"strategyqa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "863bcd3b-d3d0-495a-8a1b-e5a20fa7b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_data(f'{dataset_name}/original/{dataset_name}_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13624b5c-6789-4e70-9ace-79eaaf971c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10519688-0962-47ff-b2be-5c970b933f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(training_data) * 8 // 10\n",
    "other_size = len(training_data) // 10\n",
    "new_training_data = training_data[:train_size]\n",
    "validation_data = training_data[train_size:train_size + other_size]\n",
    "test_data = training_data[train_size + other_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce3caaf0-8b78-4e01-a3d5-57cadaf4527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dataset_name}/prompt/{dataset_name}_cot.json', 'r') as file:\n",
    "    strategyqa_examples = json.load(file)\n",
    "    \n",
    "with open('request.json', 'r') as file:\n",
    "    request_prompt = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c31975b-275c-4fa0-a5a5-8675b4d8bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategyqa_system_message, reasoning_message = reasoning_prompt_system(strategyqa_examples, request_prompt, num_shot=8)\n",
    "strategyqa_user_message = reasoning_prompt_user(new_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "272c14f8-0fe0-4bf9-a0e2-3a42e2474d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-2024-05-13/strategyqa\n",
      "The mean length of GPT input is 1039.1512008733625\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1295.1512008733625\n",
      "estimated input price is $9.518625000000002/$0.0051957560043668135\n",
      "estimated output price is $28.13952/$0.00384\n",
      "estimated final price is $37.658145000000005\n",
      "\n",
      "gpt-4-turbo-2024-04-09/strategyqa\n",
      "The mean length of GPT input is 1047.3160480349345\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1303.3160480349345\n",
      "estimated input price is $19.18683/$0.010473160480349346\n",
      "estimated output price is $56.27904/$0.00768\n",
      "estimated final price is $75.46587\n",
      "\n",
      "gpt-4-0125-preview/strategyqa\n",
      "The mean length of GPT input is 1047.3160480349345\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1303.3160480349345\n",
      "estimated input price is $19.18683/$0.010473160480349346\n",
      "estimated output price is $56.27904/$0.00768\n",
      "estimated final price is $75.46587\n",
      "\n",
      "gpt-4-0613/strategyqa\n",
      "The mean length of GPT input is 1047.3160480349345\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1303.3160480349345\n",
      "estimated input price is $57.56049/$0.03141948144104804\n",
      "estimated output price is $112.55808/$0.01536\n",
      "estimated final price is $170.11857\n",
      "\n",
      "gpt-3.5-turbo-0125/strategyqa\n",
      "The mean length of GPT input is 1047.3160480349345\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1303.3160480349345\n",
      "estimated input price is $0.9593415/$0.0005236580240174672\n",
      "estimated output price is $2.813952/$0.000384\n",
      "estimated final price is $3.7732935\n",
      "\n",
      "gpt-3.5-turbo-1106/strategyqa\n",
      "The mean length of GPT input is 1047.3160480349345\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1303.3160480349345\n",
      "estimated input price is $1.918683/$0.0010473160480349345\n",
      "estimated output price is $3.7519359999999997/$0.000512\n",
      "estimated final price is $5.670618999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strategyqa_example = wrap_reasoning_prompt(\n",
    "    model_name='gpt-4o-2024-05-13', \n",
    "    system_message=strategyqa_system_message,\n",
    "    user_message=strategyqa_user_message,\n",
    "    few_shot_message=reasoning_message,\n",
    "    num_prompts=None,\n",
    "    max_tokens=256,\n",
    "    num_sample=4,\n",
    "    temperature=0.8,\n",
    "    save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cc4aa35-e3c6-44e3-b252-224662952b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dataset_name}/prompt/valid_raw.json', 'w') as f:\n",
    "    json.dump(validation_data, f)\n",
    "\n",
    "with open(f'{dataset_name}/prompt/test_raw.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68beb041-3ab2-4c2c-8286-436eb7b2a95a",
   "metadata": {},
   "source": [
    "## OBQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3a506ec-60d9-4185-ba1e-a030b604f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"obqa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbaa0c66-e279-49b8-b3cb-d3f3cd92b726",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_data(f'{dataset_name}/original/train.jsonl')\n",
    "validation_data = read_data(f'{dataset_name}/original/dev.jsonl')\n",
    "test_data = read_data(f'{dataset_name}/original/test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "081cba57-3368-41ea-8d9b-3f6c6550553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dataset_name}/prompt/{dataset_name}_cot.json', 'r') as file:\n",
    "    obqa_examples = json.load(file)\n",
    "    \n",
    "with open('request.json', 'r') as file:\n",
    "    request_prompt = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84d02349-2a95-4f70-a6dd-d7c43c4b580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obqa_system_message, reasoning_message = reasoning_prompt_system(obqa_examples, request_prompt, num_shot=7)\n",
    "obqa_user_message = reasoning_prompt_user(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ac66223-1d82-4bf8-b388-6cf2e3f3620f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-2024-05-13/obqa\n",
      "The mean length of GPT input is 1554.7401654226346\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1810.7401654226346\n",
      "estimated input price is $38.534235/$0.007773700827113174\n",
      "estimated output price is $76.13952/$0.00384\n",
      "estimated final price is $114.673755\n",
      "\n",
      "gpt-4-turbo-2024-04-09/obqa\n",
      "The mean length of GPT input is 1577.029453298366\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1833.029453298366\n",
      "estimated input price is $78.17335000000001/$0.01577029453298366\n",
      "estimated output price is $152.27904/$0.00768\n",
      "estimated final price is $230.45239000000004\n",
      "\n",
      "gpt-4-0125-preview/obqa\n",
      "The mean length of GPT input is 1577.029453298366\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1833.029453298366\n",
      "estimated input price is $78.17335000000001/$0.01577029453298366\n",
      "estimated output price is $152.27904/$0.00768\n",
      "estimated final price is $230.45239000000004\n",
      "\n",
      "gpt-4-0613/obqa\n",
      "The mean length of GPT input is 1577.029453298366\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1833.029453298366\n",
      "estimated input price is $234.52005/$0.04731088359895098\n",
      "estimated output price is $304.55808/$0.01536\n",
      "estimated final price is $539.07813\n",
      "\n",
      "gpt-3.5-turbo-0125/obqa\n",
      "The mean length of GPT input is 1577.029453298366\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1833.029453298366\n",
      "estimated input price is $3.9086674999999995/$0.0007885147266491829\n",
      "estimated output price is $7.613952/$0.000384\n",
      "estimated final price is $11.5226195\n",
      "\n",
      "gpt-3.5-turbo-1106/obqa\n",
      "The mean length of GPT input is 1577.029453298366\n",
      "The max length of output is 256\n",
      "estimated tokens per request is 1833.029453298366\n",
      "estimated input price is $7.817334999999999/$0.0015770294532983658\n",
      "estimated output price is $10.151936/$0.000512\n",
      "estimated final price is $17.969271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obqa_example = wrap_reasoning_prompt(\n",
    "    model_name='gpt-4o-2024-05-13', \n",
    "    system_message=obqa_system_message,\n",
    "    user_message=obqa_user_message,\n",
    "    few_shot_message=reasoning_message,\n",
    "    num_prompts=None,\n",
    "    max_tokens=256,\n",
    "    num_sample=4,\n",
    "    temperature=0.8,\n",
    "    save_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "678fef86-8a12-4d4a-86bb-84a5745b04a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{dataset_name}/prompt/valid_raw.json', 'w') as f:\n",
    "    json.dump(validation_data, f)\n",
    "\n",
    "with open(f'{dataset_name}/prompt/test_raw.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b6df8-92c4-46a8-97f5-51a391e1a307",
   "metadata": {},
   "source": [
    "# Collect Reasoning Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "020ae714-bc1b-4dcd-9988-1f6e91cc243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from Levenshtein import distance as levenshtein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d098be-9817-47ea-8d34-e4f8c3436e0f",
   "metadata": {},
   "source": [
    "## Function for Processing Reasoning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1fb5ec1-193b-43c1-92d9-d00427ed36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reasoning_data(dataset_name, num_iter, num_option, num_augment):\n",
    "    train_list = []\n",
    "    retry_dataset = None\n",
    "    \n",
    "    for index in range(1, num_iter):\n",
    "        read_file = f'{dataset_name}/prompt/reasoning/train_openai_{index}_results.jsonl'\n",
    "        write_file = f'{dataset_name}/prompt/reasoning/train_openai_{index + 1}.jsonl'\n",
    "        train_dataset, retry_dataset = extract_answer_reasoning(read_file, num_option + 1, num_augment, write_file)\n",
    "        train_list += train_dataset\n",
    "        print(f\"Length of {index} data: {len(train_dataset)}\")\n",
    "        print(f\"Length of {index + 1} retry data: {len(retry_dataset)}\\n\")\n",
    "        \n",
    "    return train_list, retry_dataset\n",
    "\n",
    "\n",
    "def collate_content(content):\n",
    "    match = re.compile(r'(Key Information.*?)(Explanations:)', re.DOTALL).search(content)\n",
    "    if match:\n",
    "        general_knowledge = match.group(1).strip()\n",
    "        cleaned_content = general_knowledge + '\\n' + content[match.end(1):]\n",
    "    else:\n",
    "        cleaned_content = content\n",
    "\n",
    "    cleaned_content = re.sub(r'Explanations:\\s*', 'Explanations: ', cleaned_content)\n",
    "    return [line.strip() for line in cleaned_content.strip().split(\"\\n\") if line.strip()]\n",
    "\n",
    "\n",
    "def filter_answer(answer_list, check_num):\n",
    "    num_retry = 0\n",
    "    candidate_answer = []\n",
    "    max_tokens_double = False\n",
    "\n",
    "    answer_pattern = re.compile(r\"^[A-E] is (correct|incorrect)\\.$\")\n",
    "    \n",
    "    for item in answer_list:\n",
    "        # Skip if the response was cut off due to length\n",
    "        if item[\"finish_reason\"] == 'length':\n",
    "            num_retry += 1\n",
    "            max_tokens_double = True\n",
    "            continue\n",
    "        \n",
    "        result_content = collate_content(item['message']['content'])\n",
    "        if \"Key Information:\" not in result_content[0]:\n",
    "            # if after collation the knowledge part is still strange then skip\n",
    "            num_retry += 1\n",
    "            continue\n",
    "        \n",
    "        general_knowledge = result_content[0].split(\"Key Information:\")[1].strip()\n",
    "        if not general_knowledge.endswith(('.', '\"', \"'\")):\n",
    "            general_knowledge = general_knowledge + '.'\n",
    "        \n",
    "        answer_dict = {\n",
    "            'general_knowledge': general_knowledge,\n",
    "            'answer_prefix': [],\n",
    "            'specific_knowledge': [],\n",
    "            'LLM_answer': None\n",
    "        }\n",
    "       \n",
    "        for content in result_content[1:check_num]:\n",
    "            if \"Explanations:\" in content and \"correct\" in content:\n",
    "                content = content.split(\"Explanations: \")[-1].strip()\n",
    "            elif \"is correct\" in content or \"is incorrect\" in content:\n",
    "                content = content.strip()\n",
    "            else:\n",
    "                # if after collation the specific_knowledge part is still strange then skip\n",
    "                continue\n",
    "            \n",
    "            content_split = None\n",
    "            for delimiter in [\"Because \", \"Because, \", \"Although \", \"While \"]:\n",
    "                if delimiter in content:\n",
    "                    content_split = content.split(delimiter)\n",
    "                    break\n",
    "                    \n",
    "            if not content_split:\n",
    "                continue\n",
    "                \n",
    "            answer_prefix, reason_part = content_split\n",
    "            answer_prefix = answer_prefix.strip()\n",
    "            reason_part = reason_part.strip()\n",
    "\n",
    "            if not answer_pattern.match(answer_prefix):\n",
    "                answer_dict['LLM_answer'] = None\n",
    "                break\n",
    "            \n",
    "            if \" correct\" in answer_prefix:\n",
    "                answer_dict['LLM_answer'] = answer_prefix.split(\" is correct\")[0].strip()\n",
    "\n",
    "            answer_dict['answer_prefix'].append(answer_prefix)\n",
    "            answer_dict['specific_knowledge'].append(reason_part)\n",
    "\n",
    "        if answer_dict['LLM_answer']:\n",
    "            candidate_answer.append(answer_dict)\n",
    "        else:\n",
    "            # case-1 Sometimes all prefix will be 'is incorrect'\n",
    "            # case-2 the answer format is strange\n",
    "            num_retry += 1\n",
    "        \n",
    "    return num_retry, max_tokens_double, candidate_answer\n",
    "            \n",
    "\n",
    "def select_candidate(candidates, augment_num):\n",
    "    candidate_general_knowledge = [item['general_knowledge'] for item in candidates]\n",
    "    num_general_knowledges = len(candidate_general_knowledge)\n",
    "    dist_matrix = np.zeros((num_general_knowledges, num_general_knowledges))\n",
    "    \n",
    "    for i, j in combinations(range(num_general_knowledges), 2):\n",
    "        dist = levenshtein_distance(candidate_general_knowledge[i], candidate_general_knowledge[j])\n",
    "        dist_matrix[i, j] = dist\n",
    "        dist_matrix[j, i] = dist\n",
    "    \n",
    "    threshold = 10\n",
    "    \n",
    "    selected_general_knowledges = []\n",
    "    for i in range(num_general_knowledges):\n",
    "        if all(dist_matrix[i, j] > threshold for j in range(num_general_knowledges) if i != j):\n",
    "            selected_general_knowledges.append(candidate_general_knowledge[i])\n",
    "    \n",
    "    selected_candidates = [item for item in candidates if item['general_knowledge'] in selected_general_knowledges[:augment_num]]\n",
    "\n",
    "    if len(selected_candidates) < augment_num:\n",
    "        remaining_candidate = [item for item in candidates if item not in selected_candidates]\n",
    "        additional_content = random.sample(remaining_candidate, min(augment_num - len(selected_candidates), len(remaining_candidate)))\n",
    "        selected_candidates.extend(additional_content)\n",
    "    \n",
    "    return selected_candidates\n",
    "\n",
    "\n",
    "def extract_answer_reasoning(file_path, check_num, augment_num, retry_path):\n",
    "    item_list = []\n",
    "    retry_list = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            prompt, answer = json.loads(line)\n",
    "            \n",
    "            if isinstance(answer, list):\n",
    "                # which means the request is totally failed\n",
    "                retry_list.append(prompt)\n",
    "                continue\n",
    "            \n",
    "            user_prompt = prompt['messages'][-1]['content'].strip().split(\"\\nKey Information:\\nExplanations: \")\n",
    "            question_options = user_prompt[0].strip()\n",
    "            ground_truth = user_prompt[1].split(\" is correct\")[0].strip()\n",
    "                \n",
    "            num_retry, max_token_double, candidate_answer = filter_answer(answer['choices'], check_num)\n",
    "            if max_token_double:\n",
    "                prompt[\"max_tokens\"] *= 2\n",
    "\n",
    "            # Use vote to check if LLM agree with the ground truth\n",
    "            # if the result > 50%, then we should follow the LLM and change the ground truth\n",
    "            LLM_answer = [item['LLM_answer'] for item in candidate_answer]\n",
    "            if len(LLM_answer) == 0:\n",
    "                prompt['n'] = num_retry\n",
    "                retry_list.append(prompt)\n",
    "                continue\n",
    "            \n",
    "            most_vote_answer, _ = Counter(LLM_answer).most_common(1)[0]\n",
    "            \n",
    "            if most_vote_answer != ground_truth:\n",
    "                ground_truth = most_vote_answer\n",
    "                \n",
    "            consistent_candidates = [candidate for candidate in candidate_answer if f\"{ground_truth} is correct.\" in candidate['answer_prefix']]\n",
    "            sampled_candidates = select_candidate(consistent_candidates, augment_num)\n",
    "\n",
    "            for candidate in sampled_candidates:\n",
    "                object_dict = {\n",
    "                    'input': question_options,\n",
    "                    'GT': ground_truth,\n",
    "                    'general_knowledge': candidate['general_knowledge'],\n",
    "                    'specific_knowledge': []\n",
    "                }\n",
    "                for prefix, specific_knowledge in zip(candidate['answer_prefix'], candidate['specific_knowledge']):\n",
    "                    if not specific_knowledge.endswith(('.', '\"', \"'\")):\n",
    "                        specific_knowledge = specific_knowledge.strip() + '.'\n",
    "                    if re.match(r'^[A-Z]$', prefix[0]):\n",
    "                        object_dict['specific_knowledge'].append(f\"For option {prefix[0]}, {specific_knowledge.strip()}\")\n",
    "\n",
    "                if len(object_dict['specific_knowledge']) == check_num - 1:\n",
    "                    item_list.append(object_dict)\n",
    "                else:\n",
    "                    num_retry += 1\n",
    "                    \n",
    "            if num_retry != 0:\n",
    "                if num_retry > 8:\n",
    "                    print(\"wrong\")\n",
    "                else:\n",
    "                    prompt['n'] = num_retry\n",
    "                    retry_list.append(prompt)\n",
    "                \n",
    "    if retry_path:\n",
    "        with open(retry_path, 'w') as file:\n",
    "            for prompt in retry_list:\n",
    "                file.write(json.dumps(prompt) + '\\n')\n",
    "    \n",
    "    return item_list, retry_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b5dcf-30af-414b-8fff-420c07727657",
   "metadata": {},
   "source": [
    "## Final Process and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4171e076-52f8-43e9-b7a5-f515cb65ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save(dataset_name, train_data, save_flag=False):\n",
    "    ### preprocess train data ###\n",
    "    train_recall_data = []\n",
    "    train_analyze_data = []\n",
    "    train_summarize_data = []\n",
    "    \n",
    "    for data in train_data:\n",
    "        data_input = data['input'].replace(\"Options: \", \"\")\n",
    "        lines = data_input.split(\"\\n\")\n",
    "        lines[1] = \" \".join([f\"({item[0]}) {item[3:]}\" for item in lines[1].split(\", \")])\n",
    "        data_input = \"\\n\".join(lines)\n",
    "        \n",
    "        train_recall_data.append(\n",
    "            {\n",
    "                'input': f\"{data_input}\\nRecall:\",\n",
    "                'GT': data['GT'],\n",
    "                'recall': data['general_knowledge']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        for knowledge in data['specific_knowledge']:\n",
    "            pattern = r\"(For option [A-Z],)(.*)\"\n",
    "            matches = re.findall(pattern, knowledge)\n",
    "            prefix = matches[0][0].strip()\n",
    "            knowledge_text = matches[0][1].strip()\n",
    "            train_analyze_data.append(\n",
    "                {\n",
    "                    'input': f\"{data_input}\\nRecall: {data['general_knowledge']}\\nAnalyze: {prefix}\",\n",
    "                    'GT': data['GT'],\n",
    "                    'analyze': knowledge_text\n",
    "                }\n",
    "            )\n",
    "\n",
    "        train_summarize_data.append(\n",
    "            {\n",
    "                'input': f\"{data_input}\\nRecall: {data['general_knowledge']}\\nAnalyze: {' '.join(data['specific_knowledge'])}\\nSummarize:\",\n",
    "                'GT': data['GT'],\n",
    "                'summarize': f\"So the answer is option {data['GT']}.\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def _remove_duplicates(data_list, unique_keys):\n",
    "        seen = set()\n",
    "        unique_data_list = []\n",
    "        for data in data_list:\n",
    "            identifier = tuple(data[key] for key in unique_keys)\n",
    "            if identifier not in seen:\n",
    "                seen.add(identifier)\n",
    "                unique_data_list.append(data)\n",
    "        return unique_data_list\n",
    "\n",
    "    print(f\"Length of original recall data: {len(train_recall_data)}\")\n",
    "    train_recall_data = _remove_duplicates(train_recall_data, ['input', 'recall'])\n",
    "    print(f\"After remove duplicates data, the length of recall data: {len(train_recall_data)}\")\n",
    "    print(f\"Length of original analyze data: {len(train_analyze_data)}\")\n",
    "    train_analyze_data = _remove_duplicates(train_analyze_data, ['input', 'analyze'])\n",
    "    print(f\"After remove duplicates data, the length of analyze data: {len(train_analyze_data)}\")\n",
    "    print(f\"Length of original summarize data: {len(train_summarize_data)}\")\n",
    "    train_summarize_data = _remove_duplicates(train_summarize_data, ['input', 'summarize'])\n",
    "    print(f\"After remove duplicates data, the length of summarize data: {len(train_summarize_data)}\")\n",
    "    \n",
    "    def _preprocess(data_list):\n",
    "        item_list = []\n",
    "        for item in data_list:\n",
    "            item_choices = \" \".join([f\"({choice[0]}) {choice[3:]}\" for choice in item['Choices'].split(\", \")])\n",
    "            item_dict = {\n",
    "                \"input\": f\"{item['Q']}\\n{item_choices}\\nRecall:\",\n",
    "                \"GT\": item[\"GT\"]\n",
    "            }\n",
    "            item_list.append(item_dict)\n",
    "        return item_list\n",
    "\n",
    "\n",
    "    ### preprocess validation data ###\n",
    "    with open(f'{dataset_name}/prompt/valid_raw.json', 'r') as file:\n",
    "        valid_data = json.load(file)\n",
    "    valid_dataset = _preprocess(valid_data)\n",
    "\n",
    "\n",
    "    ### preprocess test data ###\n",
    "    with open(f'{dataset_name}/prompt/test_raw.json', 'r') as file:\n",
    "        test_data = json.load(file)\n",
    "    test_dataset = _preprocess(test_data)\n",
    "\n",
    "    \n",
    "    if save_flag:\n",
    "        with open(f'{dataset_name}/final/sft/recall_analyze_summarize/recall.json', 'w') as f:\n",
    "            json.dump(train_recall_data, f)\n",
    "        with open(f'{dataset_name}/final/sft/recall_analyze_summarize/analyze.json', 'w') as f:\n",
    "            json.dump(train_analyze_data, f)\n",
    "        with open(f'{dataset_name}/final/sft/recall_analyze_summarize/summarize.json', 'w') as f:\n",
    "            json.dump(train_summarize_data, f)\n",
    "        with open(f'{dataset_name}/final/sft/recall_analyze_summarize/valid.json', 'w') as f:\n",
    "            json.dump(valid_dataset, f)\n",
    "        with open(f'{dataset_name}/final/sft/recall_analyze_summarize/test.json', 'w') as f:\n",
    "            json.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41442506-64da-4eaf-b5dc-8cfa1d67972f",
   "metadata": {},
   "source": [
    "## CSQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61bab068-2592-4e69-8b71-c3654e306f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of 1 data: 61730\n",
      "Length of 2 retry data: 3092\n",
      "\n",
      "Length of 2 data: 5006\n",
      "Length of 3 retry data: 123\n",
      "\n",
      "Length of 3 data: 135\n",
      "Length of 4 retry data: 32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"csqa\"\n",
    "num_iter = 4\n",
    "num_option = 5\n",
    "num_augment = 8\n",
    "csqa_train, csqa_retry = read_reasoning_data(dataset_name, num_iter, num_option, num_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09249889-b425-4b0f-a2d2-6046c8bcae52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original recall data: 66871\n",
      "After reduce duplicates data, the length of recall data: 66740\n",
      "Length of original analyze data: 334355\n",
      "After reduce duplicates data, the length of analyze data: 334316\n",
      "Length of original summarize data: 66871\n",
      "After reduce duplicates data, the length of summarize data: 66870\n"
     ]
    }
   ],
   "source": [
    "process_and_save(dataset_name, csqa_train, save_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2800b85f-9f3d-4537-8a79-d9c86d4c0b4a",
   "metadata": {},
   "source": [
    "## StrategyQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa464e72-1f85-44e5-a7c8-76328b757527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of 1 data: 7123\n",
      "Length of 2 retry data: 22\n",
      "\n",
      "Length of 2 data: 22\n",
      "Length of 3 retry data: 4\n",
      "\n",
      "7145\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"strategyqa\"\n",
    "num_iter = 3\n",
    "num_option = 2\n",
    "num_augment = 4\n",
    "strategyqa_train, strategyqa_retry = read_reasoning_data(dataset_name, num_iter, num_option, num_augment)\n",
    "print(len(strategyqa_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a56f4a7-9833-42e5-a687-acc34f2b4aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original recall data: 7145\n",
      "After reduce duplicates data, the length of recall data: 7089\n",
      "Length of original analyze data: 14290\n",
      "After reduce duplicates data, the length of analyze data: 14290\n",
      "Length of original summarize data: 7145\n",
      "After reduce duplicates data, the length of summarize data: 7145\n"
     ]
    }
   ],
   "source": [
    "process_and_save(dataset_name, strategyqa_train, save_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859d7d2-20ea-43d6-96c4-0c614b8fbe39",
   "metadata": {},
   "source": [
    "## OBQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab6ed913-f717-4307-a37f-1a2237dc888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of 1 data: 18757\n",
      "Length of 2 retry data: 593\n",
      "\n",
      "Length of 2 data: 666\n",
      "Length of 3 retry data: 108\n",
      "\n",
      "Length of 3 data: 71\n",
      "Length of 4 retry data: 77\n",
      "\n",
      "19494\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"obqa\"\n",
    "num_iter = 4\n",
    "num_option = 4\n",
    "num_augment = 4\n",
    "obqa_train, obqa_retry = read_reasoning_data(dataset_name, num_iter, num_option, num_augment)\n",
    "print(len(obqa_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88adca9a-de4a-4812-b42d-ddc3b4695151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original recall data: 19494\n",
      "After reduce duplicates data, the length of recall data: 19439\n",
      "Length of original analyze data: 77976\n",
      "After reduce duplicates data, the length of analyze data: 77965\n",
      "Length of original summarize data: 19494\n",
      "After reduce duplicates data, the length of summarize data: 19492\n"
     ]
    }
   ],
   "source": [
    "process_and_save(dataset_name, obqa_train, save_flag=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
